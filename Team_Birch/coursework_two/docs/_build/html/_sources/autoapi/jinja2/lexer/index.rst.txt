jinja2.lexer
============

.. py:module:: jinja2.lexer

.. autoapi-nested-parse::

   Implements a Jinja / Python combination lexer. The ``Lexer`` class
   is used to do some preprocessing. It filters out invalid operators like
   the bitshift operators we don't allow in templates. It separates
   template code and python code in expressions.



Attributes
----------

.. autoapisummary::

   jinja2.lexer.whitespace_re
   jinja2.lexer.newline_re
   jinja2.lexer.string_re
   jinja2.lexer.integer_re
   jinja2.lexer.float_re
   jinja2.lexer.TOKEN_ADD
   jinja2.lexer.TOKEN_ASSIGN
   jinja2.lexer.TOKEN_COLON
   jinja2.lexer.TOKEN_COMMA
   jinja2.lexer.TOKEN_DIV
   jinja2.lexer.TOKEN_DOT
   jinja2.lexer.TOKEN_EQ
   jinja2.lexer.TOKEN_FLOORDIV
   jinja2.lexer.TOKEN_GT
   jinja2.lexer.TOKEN_GTEQ
   jinja2.lexer.TOKEN_LBRACE
   jinja2.lexer.TOKEN_LBRACKET
   jinja2.lexer.TOKEN_LPAREN
   jinja2.lexer.TOKEN_LT
   jinja2.lexer.TOKEN_LTEQ
   jinja2.lexer.TOKEN_MOD
   jinja2.lexer.TOKEN_MUL
   jinja2.lexer.TOKEN_NE
   jinja2.lexer.TOKEN_PIPE
   jinja2.lexer.TOKEN_POW
   jinja2.lexer.TOKEN_RBRACE
   jinja2.lexer.TOKEN_RBRACKET
   jinja2.lexer.TOKEN_RPAREN
   jinja2.lexer.TOKEN_SEMICOLON
   jinja2.lexer.TOKEN_SUB
   jinja2.lexer.TOKEN_TILDE
   jinja2.lexer.TOKEN_WHITESPACE
   jinja2.lexer.TOKEN_FLOAT
   jinja2.lexer.TOKEN_INTEGER
   jinja2.lexer.TOKEN_NAME
   jinja2.lexer.TOKEN_STRING
   jinja2.lexer.TOKEN_OPERATOR
   jinja2.lexer.TOKEN_BLOCK_BEGIN
   jinja2.lexer.TOKEN_BLOCK_END
   jinja2.lexer.TOKEN_VARIABLE_BEGIN
   jinja2.lexer.TOKEN_VARIABLE_END
   jinja2.lexer.TOKEN_RAW_BEGIN
   jinja2.lexer.TOKEN_RAW_END
   jinja2.lexer.TOKEN_COMMENT_BEGIN
   jinja2.lexer.TOKEN_COMMENT_END
   jinja2.lexer.TOKEN_COMMENT
   jinja2.lexer.TOKEN_LINESTATEMENT_BEGIN
   jinja2.lexer.TOKEN_LINESTATEMENT_END
   jinja2.lexer.TOKEN_LINECOMMENT_BEGIN
   jinja2.lexer.TOKEN_LINECOMMENT_END
   jinja2.lexer.TOKEN_LINECOMMENT
   jinja2.lexer.TOKEN_DATA
   jinja2.lexer.TOKEN_INITIAL
   jinja2.lexer.TOKEN_EOF
   jinja2.lexer.operators
   jinja2.lexer.reverse_operators
   jinja2.lexer.operator_re
   jinja2.lexer.ignored_tokens
   jinja2.lexer.ignore_if_empty


Classes
-------

.. autoapisummary::

   jinja2.lexer.Failure
   jinja2.lexer.Token
   jinja2.lexer.TokenStreamIterator
   jinja2.lexer.TokenStream
   jinja2.lexer.OptionalLStrip
   jinja2.lexer.Lexer


Functions
---------

.. autoapisummary::

   jinja2.lexer.describe_token
   jinja2.lexer.describe_token_expr
   jinja2.lexer.count_newlines
   jinja2.lexer.compile_rules
   jinja2.lexer.get_lexer


Module Contents
---------------

.. py:data:: whitespace_re

.. py:data:: newline_re

.. py:data:: string_re

.. py:data:: integer_re

.. py:data:: float_re

.. py:data:: TOKEN_ADD

.. py:data:: TOKEN_ASSIGN

.. py:data:: TOKEN_COLON

.. py:data:: TOKEN_COMMA

.. py:data:: TOKEN_DIV

.. py:data:: TOKEN_DOT

.. py:data:: TOKEN_EQ

.. py:data:: TOKEN_FLOORDIV

.. py:data:: TOKEN_GT

.. py:data:: TOKEN_GTEQ

.. py:data:: TOKEN_LBRACE

.. py:data:: TOKEN_LBRACKET

.. py:data:: TOKEN_LPAREN

.. py:data:: TOKEN_LT

.. py:data:: TOKEN_LTEQ

.. py:data:: TOKEN_MOD

.. py:data:: TOKEN_MUL

.. py:data:: TOKEN_NE

.. py:data:: TOKEN_PIPE

.. py:data:: TOKEN_POW

.. py:data:: TOKEN_RBRACE

.. py:data:: TOKEN_RBRACKET

.. py:data:: TOKEN_RPAREN

.. py:data:: TOKEN_SEMICOLON

.. py:data:: TOKEN_SUB

.. py:data:: TOKEN_TILDE

.. py:data:: TOKEN_WHITESPACE

.. py:data:: TOKEN_FLOAT

.. py:data:: TOKEN_INTEGER

.. py:data:: TOKEN_NAME

.. py:data:: TOKEN_STRING

.. py:data:: TOKEN_OPERATOR

.. py:data:: TOKEN_BLOCK_BEGIN

.. py:data:: TOKEN_BLOCK_END

.. py:data:: TOKEN_VARIABLE_BEGIN

.. py:data:: TOKEN_VARIABLE_END

.. py:data:: TOKEN_RAW_BEGIN

.. py:data:: TOKEN_RAW_END

.. py:data:: TOKEN_COMMENT_BEGIN

.. py:data:: TOKEN_COMMENT_END

.. py:data:: TOKEN_COMMENT

.. py:data:: TOKEN_LINESTATEMENT_BEGIN

.. py:data:: TOKEN_LINESTATEMENT_END

.. py:data:: TOKEN_LINECOMMENT_BEGIN

.. py:data:: TOKEN_LINECOMMENT_END

.. py:data:: TOKEN_LINECOMMENT

.. py:data:: TOKEN_DATA

.. py:data:: TOKEN_INITIAL

.. py:data:: TOKEN_EOF

.. py:data:: operators

.. py:data:: reverse_operators

.. py:data:: operator_re

.. py:data:: ignored_tokens

.. py:data:: ignore_if_empty

.. py:function:: describe_token(token: Token) -> str

   Returns a description of the token.


.. py:function:: describe_token_expr(expr: str) -> str

   Like `describe_token` but for token expressions.


.. py:function:: count_newlines(value: str) -> int

   Count the number of newline characters in the string.  This is
   useful for extensions that filter a stream.


.. py:function:: compile_rules(environment: jinja2.environment.Environment) -> List[Tuple[str, str]]

   Compiles all the rules from the environment into a list of rules.


.. py:class:: Failure(message: str, cls: Type[jinja2.exceptions.TemplateSyntaxError] = TemplateSyntaxError)

   Class that raises a `TemplateSyntaxError` if called.
   Used by the `Lexer` to specify known errors.


   .. py:attribute:: message


   .. py:attribute:: error_class


.. py:class:: Token

   Bases: :py:obj:`NamedTuple`


   .. py:attribute:: lineno
      :type:  int


   .. py:attribute:: type
      :type:  str


   .. py:attribute:: value
      :type:  str


   .. py:method:: test(expr: str) -> bool

      Test a token against a token expression.  This can either be a
      token type or ``'token_type:token_value'``.  This can only test
      against string values and types.



   .. py:method:: test_any(*iterable: str) -> bool

      Test against multiple token expressions.



.. py:class:: TokenStreamIterator(stream: TokenStream)

   The iterator for tokenstreams.  Iterate over the stream
   until the eof token is reached.


   .. py:attribute:: stream


.. py:class:: TokenStream(generator: Iterable[Token], name: Optional[str], filename: Optional[str])

   A token stream is an iterable that yields :class:`Token`\s.  The
   parser however does not iterate over it but calls :meth:`next` to go
   one token ahead.  The current active token is stored as :attr:`current`.


   .. py:attribute:: name


   .. py:attribute:: filename


   .. py:attribute:: closed
      :value: False



   .. py:attribute:: current


   .. py:property:: eos
      :type: bool


      Are we at the end of the stream?


   .. py:method:: push(token: Token) -> None

      Push a token back to the stream.



   .. py:method:: look() -> Token

      Look at the next token.



   .. py:method:: skip(n: int = 1) -> None

      Got n tokens ahead.



   .. py:method:: next_if(expr: str) -> Optional[Token]

      Perform the token test and return the token if it matched.
      Otherwise the return value is `None`.



   .. py:method:: skip_if(expr: str) -> bool

      Like :meth:`next_if` but only returns `True` or `False`.



   .. py:method:: close() -> None

      Close the stream.



   .. py:method:: expect(expr: str) -> Token

      Expect a given token type and return it.  This accepts the same
      argument as :meth:`jinja2.lexer.Token.test`.



.. py:function:: get_lexer(environment: jinja2.environment.Environment) -> Lexer

   Return a lexer which is probably cached.


.. py:class:: OptionalLStrip

   Bases: :py:obj:`tuple`


   A special tuple for marking a point in the state that can have
   lstrip applied.


.. py:class:: Lexer(environment: jinja2.environment.Environment)

   Class that implements a lexer for a given environment. Automatically
   created by the environment class, usually you don't have to do that.

   Note that the lexer is not automatically bound to an environment.
   Multiple environments can share the same lexer.


   .. py:attribute:: lstrip_blocks


   .. py:attribute:: newline_sequence


   .. py:attribute:: keep_trailing_newline


   .. py:attribute:: rules
      :type:  Dict[str, List[_Rule]]


   .. py:method:: tokenize(source: str, name: Optional[str] = None, filename: Optional[str] = None, state: Optional[str] = None) -> TokenStream

      Calls tokeniter + tokenize and wraps it in a token stream.



   .. py:method:: wrap(stream: Iterable[Tuple[int, str, str]], name: Optional[str] = None, filename: Optional[str] = None) -> Iterator[Token]

      This is called with the stream as returned by `tokenize` and wraps
      every token in a :class:`Token` and converts the value.



   .. py:method:: tokeniter(source: str, name: Optional[str], filename: Optional[str] = None, state: Optional[str] = None) -> Iterator[Tuple[int, str, str]]

      This method tokenizes the text and returns the tokens in a
      generator. Use this method if you just want to tokenize a template.

      .. versionchanged:: 3.0
          Only ``\n``, ``\r\n`` and ``\r`` are treated as line
          breaks.



