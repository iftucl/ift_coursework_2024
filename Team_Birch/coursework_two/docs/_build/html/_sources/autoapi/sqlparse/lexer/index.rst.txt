sqlparse.lexer
==============

.. py:module:: sqlparse.lexer

.. autoapi-nested-parse::

   SQL Lexer



Classes
-------

.. autoapisummary::

   sqlparse.lexer.Lexer


Functions
---------

.. autoapisummary::

   sqlparse.lexer.tokenize


Module Contents
---------------

.. py:class:: Lexer

   The Lexer supports configurable syntax.
   To add support for additional keywords, use the `add_keywords` method.


   .. py:method:: get_default_instance()
      :classmethod:


      Returns the lexer instance used internally
      by the sqlparse core functions.



   .. py:method:: default_initialization()

      Initialize the lexer with default dictionaries.
      Useful if you need to revert custom syntax settings.



   .. py:method:: clear()

      Clear all syntax configurations.
      Useful if you want to load a reduced set of syntax configurations.
      After this call, regexps and keyword dictionaries need to be loaded
      to make the lexer functional again.



   .. py:method:: set_SQL_REGEX(SQL_REGEX)

      Set the list of regex that will parse the SQL.



   .. py:method:: add_keywords(keywords)

      Add keyword dictionaries. Keywords are looked up in the same order
      that dictionaries were added.



   .. py:method:: is_keyword(value)

      Checks for a keyword.

      If the given value is in one of the KEYWORDS_* dictionary
      it's considered a keyword. Otherwise, tokens.Name is returned.



   .. py:method:: get_tokens(text, encoding=None)

      Return an iterable of (tokentype, value) pairs generated from
      `text`. If `unfiltered` is set to `True`, the filtering mechanism
      is bypassed even if filters are defined.

      Also preprocess the text, i.e. expand tabs and strip it if
      wanted and applies registered filters.

      Split ``text`` into (tokentype, text) pairs.

      ``stack`` is the initial stack (default: ``['root']``)



.. py:function:: tokenize(sql, encoding=None)

   Tokenize sql.

   Tokenize *sql* using the :class:`Lexer` and return a 2-tuple stream
   of ``(token type, value)`` items.


