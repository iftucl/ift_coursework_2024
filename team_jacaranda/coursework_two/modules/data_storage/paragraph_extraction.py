import os
import re
import json
import datetime
import psycopg2
import pdfplumber
from minio import Minio
from fuzzywuzzy import fuzz
from pathlib import Path
from multiprocessing import Pool
from tqdm import tqdm
import psutil

# === MinIO é…ç½® ===
MINIO_ENDPOINT = 'localhost:9000'
MINIO_ACCESS_KEY = 'ift_bigdata'
MINIO_SECRET_KEY = 'minio_password'
MINIO_BUCKET = 'csreport'

# === PostgreSQL é…ç½® ===
db_config = {
    "dbname": "fift",
    "user": "postgres",
    "password": "postgres",
    "host": "host.docker.internal",
    "port": 5439
}

# === åˆå§‹åŒ– MinIO å®¢æˆ·ç«¯ ===
minio_client = Minio(
    MINIO_ENDPOINT,
    access_key=MINIO_ACCESS_KEY,
    secret_key=MINIO_SECRET_KEY,
    secure=False
)

def parse_security_and_year(object_name):
    filename = Path(object_name).name
    match = re.match(r'(.+?)_(\d{4})\.pdf', filename, re.IGNORECASE)
    if match:
        return match.group(1), int(match.group(2))
    return None, None

def extract_paragraphs_from_pdf(file_path):
    paragraphs = []
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                clean_text = re.sub(r'\s+', ' ', text)
                splits = re.split(r'(?<=[ã€‚ï¼›.\n])\s*', clean_text)
                for para in splits:
                    if len(para.strip()) > 20:
                        paragraphs.append((page.page_number, para.strip()))
    return paragraphs

def find_matching_paragraphs(paragraphs, keywords, threshold=80):
    matched = []
    for page_num, para in paragraphs:
        match_count = sum(fuzz.partial_ratio(kw.lower(), para.lower()) >= threshold for kw in keywords)
        if match_count >= 2:
            matched.append({"page": page_num, "text": para})
    return matched

def load_indicators_from_db(conn):
    with conn.cursor() as cur:
        cur.execute("SELECT indicator_id, indicator_name, keywords FROM csr_reporting.CSR_indicators")
        return cur.fetchall()

def insert_matched_data(conn, security, report_year, indicator_id, indicator_name, matched, extraction_time):
    with conn.cursor() as cur:
        cur.execute(""" 
            INSERT INTO csr_reporting.CSR_Data (
                security, report_year, indicator_id, indicator_name,
                source_excerpt, extraction_time
            )
            VALUES (%s, %s, %s, %s, %s::jsonb, %s)
        """, (
            security, report_year, indicator_id, indicator_name,
            json.dumps(matched), extraction_time
        ))
    conn.commit()

def check_memory_usage(threshold_percent=90):
    mem = psutil.virtual_memory()
    if mem.percent > threshold_percent:
        tqdm.write(f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼š{mem.percent}%")
        return False
    return True

# === å­è¿›ç¨‹å¤„ç†æŠ¥å‘Šï¼Œå¹¶è¿”å›å¤±è´¥å¯¹è±¡åï¼ˆè‹¥æœ‰ï¼‰ ===
def process_report(args):
    from tqdm import tqdm
    conn_config, obj, indicators = args
    conn = psycopg2.connect(**conn_config)
    object_name = obj.object_name
    security, report_year = parse_security_and_year(object_name)
    if not security or not report_year:
        return object_name

    local_file = f"/tmp/{security}_{report_year}.pdf"
    try:
        tqdm.write(f"ğŸ“¥ ä¸‹è½½ï¼š{object_name}")
        minio_client.fget_object(MINIO_BUCKET, object_name, local_file)

        paragraphs = extract_paragraphs_from_pdf(local_file)
        extraction_time = datetime.datetime.now()

        for indicator_id, indicator_name, keyword_list in indicators:
            keywords = keyword_list or []
            matched = find_matching_paragraphs(paragraphs, keywords)
            if matched:
                tqdm.write(f"âœ… åŒ¹é…åˆ°æŒ‡æ ‡ã€{indicator_name}ã€‘åœ¨ {security} ({report_year}) - æ®µè½æ•°: {len(matched)}")
                insert_matched_data(conn, security, report_year, indicator_id, indicator_name, matched, extraction_time)

    except Exception as e:
        tqdm.write(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ï¼š{object_name}ï¼Œé”™è¯¯ï¼š{e}")
        return object_name  # è¿”å›å¤±è´¥çš„å¯¹è±¡å
    finally:
        if os.path.exists(local_file):
            os.remove(local_file)
        conn.close()

    return None  # æˆåŠŸå¤„ç†è¿”å› None

def process_all_pdfs():
    conn_config = db_config

    with psycopg2.connect(**conn_config) as conn:
        indicators = load_indicators_from_db(conn)

    objects = list(minio_client.list_objects(MINIO_BUCKET, recursive=True))

    pool_size = 8
    batch_size = 80
    total_batches = (len(objects) + batch_size - 1) // batch_size

    tqdm.write(f"ğŸ“Š æ€»å…±éœ€è¦å¤„ç† {len(objects)} ä¸ªæ–‡ä»¶ï¼Œåˆ†ä¸º {total_batches} ä¸ª batchã€‚")

    total_progress = tqdm(total=len(objects), desc="æ€»ä½“å¤„ç†è¿›åº¦")
    failed_files = []

    for i in range(0, len(objects), batch_size):
        batch = objects[i:i+batch_size]
        with Pool(pool_size) as pool:
            for result in pool.imap_unordered(
                process_report, [(conn_config, obj, indicators) for obj in batch]
            ):
                total_progress.update(1)
                if result:
                    failed_files.append(result)

    total_progress.close()

    # å†™å…¥å¤±è´¥æ–‡ä»¶åˆ—è¡¨
    if failed_files:
        with open("failed_reports.json", "w", encoding="utf-8") as f:
            json.dump(failed_files, f, ensure_ascii=False, indent=2)
        tqdm.write(f"â— å¤„ç†å¤±è´¥çš„æ–‡ä»¶å·²å†™å…¥ failed_reports.jsonï¼Œå…± {len(failed_files)} ä¸ªã€‚")
    else:
        tqdm.write("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†æˆåŠŸï¼Œæ²¡æœ‰å¤±è´¥é¡¹ã€‚")

if __name__ == "__main__":
    process_all_pdfs()
