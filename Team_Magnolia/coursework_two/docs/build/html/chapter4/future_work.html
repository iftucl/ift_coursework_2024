<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Future Work &#8212; Magnolia ESG Extraction 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=f2a433a1"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Conclusion" href="../conclusion.html" />
    <link rel="prev" title="Limitations" href="limitations.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="future-work">
<h1>Future Work<a class="headerlink" href="#future-work" title="Link to this heading">¶</a></h1>
<hr class="docutils" />
<section id="id1">
<h2>Future Work<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h2>
<p>The current CSR data extraction and processing pipeline demonstrates robust performance across ingestion, extraction, validation, and storage stages. However, to ensure long-term scalability, maintainability, and technological competitiveness, several avenues for future enhancement have been identified. This chapter outlines technical, model-level, and infrastructural improvements, as well as potential extensions into commercial and academic domains.</p>
<section id="technical-enhancements">
<h3>Technical Enhancements<a class="headerlink" href="#technical-enhancements" title="Link to this heading">¶</a></h3>
<p>To support the expanding volume of CSR reports and increasing complexity of extraction tasks, the technical foundation of the system must evolve. Future enhancements focus on concurrency, deployment automation, and advanced error monitoring.
Concurrency and Asynchronous Processing
The current implementation of the CSR data extraction pipeline processes corporate reports sequentially. While sufficient for small-scale testing, this approach imposes significant constraints on throughput and scalability as the volume of documents increases. To alleviate this bottleneck and prepare the system for larger-scale deployments, future iterations will implement asynchronous and concurrent processing techniques, leveraging Python’s asyncio framework alongside distributed task queues such as Celery.
The core design enhancement lies in decoupling the processing stages—namely, file downloading, page filtering, LLM-based extraction, and data validation—so that they can be executed in parallel. By replacing blocking operations with non-blocking asynchronous functions, the pipeline will be capable of processing multiple reports concurrently, rather than waiting for each file to complete before initiating the next. This is particularly advantageous in I/O-bound stages such as accessing MinIO object storage or querying the MongoDB database.
Preliminary benchmarks suggest that the adoption of concurrency can yield substantial performance improvements, with a projected throughput increase of up to tenfold relative to the current serial implementation. These gains are especially critical when handling annual reports from hundreds of multinational corporations, where processing delays may otherwise become prohibitive.
The proposed implementation plan consists of three coordinated components. First, asynchronous functions (async def) will be applied to all I/O-intensive tasks to maximize the benefits of event-loop scheduling. Second, Celery will be integrated to distribute CPU-intensive workloads—particularly the dual LLM passes—across worker processes or machines. Third, a backpressure control mechanism will be introduced to monitor system memory and CPU utilization in real time, dynamically throttling the number of concurrently active tasks to avoid resource exhaustion.
By incorporating these concurrency strategies, the pipeline will become significantly more responsive and resilient under load, ensuring robust performance even as data volumes scale in future deployment scenarios.
Containerization and Automated Deployment
To ensure long-term scalability, reproducibility, and ease of maintenance, the next evolution of the CSR data extraction pipeline will focus on the full containerization of all system components, coupled with automated orchestration and scheduled deployments. While the current implementation already leverages Docker for local development, the production-grade deployment demands more robust orchestration mechanisms.
Docker Compose Enhancements
A key enhancement will be the modularization of the docker-compose.yml configuration. Rather than defining services in a monolithic manner, the system will be restructured into discrete service modules, each representing a core component of the architecture—namely MinIO (object storage), MongoDB (database), the LLM-based extraction service, and the Streamlit-based visualization application. This modularity will enable teams to independently scale or debug specific services without disrupting the entire system.
Kubernetes Migration
In environments where horizontal scalability is critical, the system will migrate from Docker Compose to a Kubernetes-based orchestration framework. Kubernetes offers built-in features such as automated service discovery, load balancing, and container auto-scaling, which will significantly improve performance under increased data volumes or concurrent workloads. Furthermore, it provides native support for resource isolation, rolling updates, and fault tolerance—attributes that are essential for enterprise-level deployment.
Airflow Scheduling Integration
In parallel, the integration of Apache Airflow is proposed to introduce intelligent scheduling of extraction workflows. Airflow will allow cron-like automation of key tasks such as report ingestion, lineage tracking, and periodic data refreshes. For example, an Airflow DAG (Directed Acyclic Graph) can be defined to trigger the entire pipeline at weekly or monthly intervals, ensuring that the system automatically ingests the latest CSR disclosures from companies without requiring manual intervention. In the event of task failures, Airflow’s retry logic and alerting mechanisms will offer robustness and transparency.
Together, this containerization and orchestration strategy will not only improve the system’s scalability but also bolster disaster recovery capabilities. Services can be redeployed quickly in the event of a crash, and infrastructure can be moved between environments—such as from local machines to cloud clusters—with minimal configuration changes.
Advanced Error Monitoring and Logging
While the current pipeline includes basic error logging—such as recording validation failures and extraction errors in MongoDB—this level of logging is largely passive and retrospective. To support a more resilient and production-ready system, a comprehensive observability framework will be introduced. This framework aims to provide real-time monitoring, alerting, and diagnostics, making system maintenance proactive rather than reactive.
One of the proposed enhancements is the integration of Sentry, an open-source tool for real-time error tracking. Sentry captures runtime exceptions, stack traces, and context variables, sending immediate notifications to the development team via email or messaging integrations such as Slack. This capability is particularly valuable for monitoring the LLM inference stage, where subtle parsing errors or unexpected prompt failures can degrade output quality. By surfacing these issues promptly, developers can investigate and resolve them before they affect downstream analyses.
In addition to error tracking, Prometheus and Grafana will be deployed to monitor system-level and pipeline-specific metrics. Prometheus will collect time-series data such as CPU and memory usage, task execution durations, success/failure rates of batch jobs, and throughput of document ingestion. These metrics will then be visualized through Grafana dashboards, offering a real-time overview of system health. Users can drill down into specific time intervals, services, or host machines to identify anomalies or performance bottlenecks.
To complement these visual tools, threshold-based alerts will be configured. For example, if the validation error rate exceeds 10% over a rolling window of 30 minutes, an alert will be triggered. Alerts can be sent via SMTP, Slack, or PagerDuty integrations. This mechanism ensures that performance regressions or data quality issues are not only detected but actively communicated to relevant personnel in a timely manner.
Altogether, this observability framework will foster greater operational confidence in the pipeline. It will empower data engineers and analysts with the visibility needed to maintain high data quality, uphold service availability, and respond swiftly to system anomalies.</p>
</section>
<section id="extraction-model-optimization">
<h3>Extraction Model Optimization<a class="headerlink" href="#extraction-model-optimization" title="Link to this heading">¶</a></h3>
<p>As the diversity, complexity, and volume of CSR reports continue to expand, the demands placed upon the data extraction component of the pipeline will intensify. The current reliance on generic, externally hosted large language models (LLMs) offers strong baseline performance, but future developments will focus on enhancing model specificity, autonomy, and cost-efficiency. This section outlines three main directions for extraction model optimization: fine-tuning an open-source LLM, advancing prompt engineering practices, and integrating OCR capabilities to handle non-standard input formats.
Fine-Tuning a 7B Parameter Open Model
To increase the system’s precision and autonomy, the pipeline will transition from proprietary APIs to an in-house fine-tuned open model. Candidates for this role include Mistral-7B and LLaMA3-7B, which strike a balance between parameter size and inference efficiency (Touvron et al., 2023). Fine-tuning such a model specifically for CSR report extraction will reduce dependency on third-party services while improving the ability to generalize across noisy, domain-specific formats.
The training dataset will be derived from the pipeline’s own outputs, specifically the final standardized JSON files (final_standardized.json). These records, already validated and mapped to canonical indicator IDs, serve as distilled supervision data. Using this structured corpus, the model will be trained to perform several tasks simultaneously: identify relevant metrics in unstructured text, map them to standard indicator slugs, infer missing fields such as units or target years, and determine the appropriate classification (metric or target).
The expected benefits of this fine-tuning initiative are multifold. First, the system will gain full control over its core extraction model, eliminating reliance on costly or opaque third-party APIs. Second, a fine-tuned model is likely to adapt more effectively to the linguistic nuances and tabular irregularities of CSR documents. Third, by deploying a smaller but specialized model, inference speed and throughput can be significantly improved, allowing for faster batch processing and lower infrastructure requirements.
Advanced Prompt Engineering Enhancements
Before full fine-tuning is completed, substantial improvements can still be achieved through enhanced prompt engineering. Current prompts are generalized and static, which may limit performance across documents from highly heterogeneous sectors. A more sophisticated approach involves developing sector-specific prompt templates tailored to the stylistic and terminological patterns of industries such as energy, finance, healthcare, and manufacturing.
Incorporating few-shot learning into prompt design will further improve the model’s contextual understanding. By including carefully selected examples—such as complex table structures or nuanced sustainability commitments—the model can better generalize to similar inputs. Additionally, prompts can be made adaptive by referencing document metadata. For instance, if a report is detected to be particularly long, or its language identified as formal or archaic, the prompt can be adjusted dynamically to increase the model’s focus or verbosity.
Collectively, these prompt enhancements will improve robustness, especially when dealing with edge cases that deviate from typical reporting styles. They also serve as a bridge towards model fine-tuning by incrementally aligning the LLM’s behavior with domain-specific expectations.
Non-Standard Input Handling via OCR Integration
A significant proportion of CSR reports, particularly from smaller firms or earlier years, are distributed as scanned image-based PDFs with no embedded text layer. These documents present a challenge for text-based LLMs, which require digital text input. To address this, the pipeline will integrate OCR (Optical Character Recognition) technologies such as Tesseract (Mao et al., 2021).
The integration process will consist of three stages. First, scanned PDFs will be detected using MIME type or metadata inspection. Second, Tesseract will be employed to convert these image-based reports into text-rich formats. The extracted text will then undergo preprocessing to reconstruct probable table layouts and identify contextual cues like headers and units. Finally, the OCR outputs will be validated against known field structures to ensure extraction accuracy.
Although OCR introduces new sources of noise—including misrecognized characters and distorted layouts—these can be mitigated through post-processing and heuristic validation. More importantly, the ability to process scanned documents significantly broadens the system’s applicability and inclusivity, allowing it to handle legacy reports and poorly digitized disclosures.</p>
</section>
<section id="infrastructure-evolution">
<h3>Infrastructure Evolution<a class="headerlink" href="#infrastructure-evolution" title="Link to this heading">¶</a></h3>
<p>As the data processed by the pipeline grows in volume and complexity, the underlying infrastructure must evolve to support analytical needs, maintain version control, and offer auditability. The current use of MongoDB, while effective for semi-structured storage, does not optimally support time-travel queries or large-scale batch analytics. To address these limitations, migration to Apache Iceberg is proposed.
Migration to Apache Iceberg for Time-Travel Queries
Apache Iceberg is a high-performance table format designed specifically for large-scale analytical workloads. It introduces capabilities that are essential for enterprise-grade sustainability data management. Chief among these is support for time-travel queries—allowing users to retrieve the state of a dataset at any historical point. This feature is especially relevant for CSR analysis, where companies may revise or restate their disclosures retroactively.
The migration plan involves transferring validated records from MongoDB into Iceberg-compatible tables, which will be stored in object storage environments such as MinIO or Amazon S3. To ensure data consistency, delta ingestion pipelines will be implemented to sync incremental updates and backfill corrections. A lightweight metadata catalog will track schema versions, ingestion timestamps, and record lineage, enabling users to trace data changes over time.
The benefits of this transition are substantial. It enables complete auditability and reproducibility of historical disclosures, facilitates longitudinal analyses across multiple reporting periods, and positions the pipeline for seamless integration with high-performance query engines such as Apache Trino, Apache Spark, and Snowflake. These advancements will make the CSR data platform analytically powerful and future-ready (Ryan et al., 2023).</p>
</section>
<section id="integration-and-commercial-applications">
<h3>Integration and Commercial Applications<a class="headerlink" href="#integration-and-commercial-applications" title="Link to this heading">¶</a></h3>
<p>In addition to technical improvements, the CSR data pipeline has the potential to evolve into a commercially viable and socially impactful platform. Future directions include integration with existing ESG systems, the development of academic research tools, and the launch of a full-featured SaaS offering.
ESG Platform Integration
Standardized CSR indicators extracted by the pipeline can be delivered as structured APIs for direct consumption by Environmental, Social, and Governance (ESG) rating agencies such as MSCI and Sustainalytics. These APIs would allow institutional clients to query data by company, year, indicator type, or theme (e.g., emissions, diversity, or energy use). In doing so, the project could become a trusted intermediary between raw disclosures and ESG analytics platforms, streamlining workflows for data acquisition and comparison.
Research and Educational Platform
Beyond commercial use, the pipeline can serve as an open-access tool for academic research and teaching. Universities could use the system to explore trends in corporate sustainability, test hypothesis-driven models, or examine data governance in real-world contexts. With full metadata and lineage records available, the platform would also exemplify best practices in data transparency, supporting coursework in fields such as environmental policy, data science, and business ethics.
SaaS Model Development
Finally, a long-term vision is to develop the system into a cloud-based Software-as-a-Service (SaaS) platform. This platform would offer subscription-based access to dynamic dashboards, custom KPI reports, and real-time alerts triggered by new CSR disclosures. Stakeholders such as regulatory bodies, NGOs, consultancy firms, and corporations themselves could benefit from tailored insights, compliance monitoring, and benchmarking tools. With scalable infrastructure, API support, and an intuitive user interface, the SaaS version of this project could provide lasting value across industries and geographies.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Magnolia ESG Extraction</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1/feasibility.html">Feasibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1/indicator_framework.html">Indicator Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1/architectural_motivation.html">Architectural Motivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter1/innovations.html">Innovations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2/workflow.html">Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2/page_filtering.html">Page Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2/two_pass_extraction.html">Two Pass Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter2/batch_processing.html">Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/validation_framework.html">Validation Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/fallback_heuristics.html">Fallback Heuristics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/error_logging.html">Error Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="case_study.html">Case Study</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="scalability.html">Scalability</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Future Work</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">Future Work</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../conclusion.html">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/architecture_overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendices/api_reference.html">Api Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="limitations.html" title="previous chapter">Limitations</a></li>
      <li>Next: <a href="../conclusion.html" title="next chapter">Conclusion</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.1</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/chapter4/future_work.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>