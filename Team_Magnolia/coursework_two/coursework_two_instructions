Full Coursework Two Assignment Brief:

Assignment Overview


This assignment forms the core of Coursework Two and simulates a real-world data product development scenario. Building on the data lake constructed in Coursework One, students will work in teams to design, implement, and deliver a functional data product focused on Corporate Social Responsibility (CSR) indicators extracted from company CSR reports.
The primary objective is to create a structured, reliable, and scalable data storage solution for CSR indicators, enabling efficient data retrieval, validation, and analysis.
Goals

Leveraging the company reports extracted in Coursework One, the goal of Coursework Two is to design, develop, and implement a functional data product based on CSR (Corporate Social Responsibility) indicators.

The key goals of Coursework Two are to:
Design, develop, and implement a functional data product based on CSR indicators.
Efficiently design the storage of these indicators to facilitate data retrieval, validation, and analysis.
Gain hands-on experience in data engineering principles and cross-functional team collaboration.
Understand the full lifecycle of a data product, from conception to delivery.
Additionally, you will efficiently design the storage of these indicators to facilitate data retrieval, validation, and analysis.

By completing this coursework, you and your team will:
Act, collaborate, and work effectively as a team.
Apply data engineering principles in a practical setting.
Gain experience in roles such as data product owner, data product specialist, and developer.
Learn to collaborate in a cross-functional team environment.
Understand the end-to-end lifecycle of a data product.
Team Structure



The team composition will follow the same allocations as in Coursework One. Teams are free to allocate resources to the following roles:
Data Product Owners: Define the strategy and vision for the data product.
Data Product Specialists: Identify and validate CSR indicators and metadata.
Developers (including data engineers and software developers): Implement the technical solution for data extraction, storage, and visualization.
Each team will decide how to assign roles and balance responsibilities among team members.


Sustainability Indicators Identification


Data specialists and product owners will identify the key strategy of the data product and collaborate with data engineers and developers to perform the following tasks:

1. Sustainability Indicators Identification
Identify the main sustainability thematic areas (e.g., deforestation, water consumption, plastic reduction, energy).
For each thematic area, determine the relevant sustainability indicators, key values, metrics, and comments.
Create a data catalogue, data dictionary, and data lineage for each indicator.
Define validation rules to ensure data quality and integrity during extraction.
2. Infrastructure Design
Data Availability Check: Identify if a company report is available in the data lake constructed in Coursework One.
Content Extraction: If a report is available, read and extract the main indicators and metadata.
Storage Design: Design a coherent, scalable, and efficient solution to persistently store the extracted data in a database of your choice.
Visualization Tool: Create the specification and basic requirements for the developers so that they can develop a visualization tool to perform data checks and analytics on the extracted data.
Infrastructure Design: Minimum Specifications


For each company stored in the PostgreSQL database `fift`, within the schema `csr_reporting` and the table `company_static`, your solution must address the following:
Data Availability: Check if a company report exists in the data lake.
Content Extraction: Extract the main indicators and metadata from the report.
Storage Design: Design a database solution that is scalable, efficient, and supports easy data retrieval and validation.
Visualization: Develop a tool to visualize and analyze the extracted data.
Infrastructure Design
The infrastructure for this project must be designed to not only store CSR data but also ensure data integrity, validation, and accommodate future updates. CSR indicators often have dynamic characteristics, such as:
Different Aims: Indicators may aim for reduction (e.g., carbon emissions) or increase (e.g., renewable energy usage).
Targets: Companies may set future targets (e.g., "reduce water consumption by 20% by 2030") that require updates as progress is made.
Updates: Companies may need to revise previously reported data or targets based on new information or changing circumstances.
The infrastructure should be scalable, efficient, and flexible to handle these complexities while supporting future analytics and reporting needs.
Design Considerations
1. Data Storage and Schema Design
Schemas and Tables: Design appropriate schemas, tables, and collections in `MongoDB` and `PostgreSQL` to support the project goals.
Dynamic Indicators: Ensure the storage solution can accommodate CSR indicators with varying aims (e.g., reduction, increase) and future targets.
Updates and Revisions: Implement mechanisms to allow updates to previously stored data, such as versioning or audit logs.
2. Data Integrity and Validation
Validation Rules: Define and implement validation rules to ensure data quality and integrity during ingestion and processing.
Metadata Management: Use metadata management systems to track data lineage, sources, and transformations.
Data Quality Tools: Integrate tools for automated data quality checks and error handling.
3. Scalability and Flexibility
Scalable Storage: Ensure the infrastructure can handle increasing volumes of CSR data as more companies and indicators are added.
Flexibility: Design the system to support evolving requirements, such as new CSR indicators or changes in reporting standards.
Code Submission
For each coursework submission, code must be submitted to GitHub via pull request assigned to [@uceslc0](https://github.com/uceslc0) for final review before merging.

The repository for submission is https://github.com/iftucl/ift_coursework_2024.



In order to submit the coursework, the students must follow the following steps:



1. Fork the repository;

2. Create a new branch;

3. Add your own developments in a dedicated folder;

4. The dedicated folder has to be placed under `./ift_coursework_2024/` and is structured as follows:



```

team_<insert your team id>

    ├── CHANGELOG.md

    ├── coursework_one/

        └── .gitkeep

        ├── config/

            ├── conf.yaml

        ├── modules/

        ├── static/

        ├── test/

        ├── Main.*

        ├── pyproject.toml

        ├── .gitkeep

        └── README.md

    ├── coursework_two/

        └── .gitkeep

        ├── config/

            ├── conf.yaml

        ├── modules/

        ├── static/

        ├── test/

        ├── Main.*

        ├── pyproject.toml

        ├── .gitkeep

        └── README.md

```

Subfolder `./modules` can be further structured in sub-folders named after what they contain. An example could be:



```

├── modules/

    ├──db/

        └── db_connection.*

    ├── input/

        └── input_reader.*

    ├── output/

        ├── script_purposes.*

        └── etc..etc..

```

Do not copy databases in other folders. There is one source only for databases and this is in folder `000.Database`.



**In addition, un-stage any change to the `000.Database` folder before committing to Git.**

Any change outside your group folder committed to git will make the pull request invalid.

Package Management
Tool: Use `Poetry` for dependency management and project packaging.
Requirements:
Initialize your project with:

`poetry init`
Manage dependencies using:

`poetry add <package>` # Install a package
`poetry remove <package>` # Remove a package
Maintain a clean `pyproject.toml` file with explicit versioning for all dependencies.
Use `poetry install` to set up the project environment for collaborators.
Best Practices:
Separate development dependencies (e.g., `pytest`, `flake8`) from production dependencies.
Pin versions for critical packages to ensure reproducibility.
Application Flexibility
Scheduling:
Design the application to support flexible execution frequencies (daily, weekly, monthly) using:
Command-line arguments (e.g., `--frequency daily`).
Configuration files (e.g., YAML/JSON files specifying `run_date` or `frequency`).
For advanced scheduling (e.g., cron-like jobs), use `APScheduler` or integrate with `Airflow`.
Parameterize time-sensitive operations (e.g., date ranges, report intervals) to avoid hardcoding.
Example:

```python
from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("--frequency", choices=["daily", "weekly", "monthly"], required=True)
args = parser.parse_args()
```
Testing
Framework: Use `pytest` for all testing activities.
Requirements:
Unit Tests: Validate individual functions and methods.
Integration Tests: Verify interactions between components (e.g., database connections, API calls).
End-to-End Tests: Test the entire pipeline from data ingestion to storage.
Coverage: Achieve ≥80% test coverage (enforce with `pytest-cov`).
Execution:

```bash
poetry run pytest ./tests/ --cov=./src/ --cov-report=term-missing
```
Best Practices:
Store tests in a `/tests` directory mirroring the project structure.
Use fixtures for reusable test components (e.g., mock databases).
Code Quality
Linting:
Use `flake8` for linting Python code

```bash
poetry run flake8 ./src/
```
Formatting:
Apply `Black` for consistent code style

```bash
poetry run black ./src/
```
Use `isort` to standardize import sorting

```bash
poetry run isort ./src/
```
Pre-commit Hooks:
Configure Git pre-commit hooks to automate linting and formatting.
Security
Vulnerability Scans:
Use `Bandit` to detect security issues in code:

```bash
poetry run bandit -r ./src/
```
Use `Safety` to check dependencies for known vulnerabilities:

```bash
poetry run safety check
```
Best Practices:
Update dependencies regularly with `poetry update`.
Store secrets (e.g., API keys) in environment variables or encrypted configs.
Documentation
Tool: Use `Sphinx` for comprehensive documentation.
Requirements:
Write Google-style or Sphinx-compatible docstrings for all modules, classes, and functions.
Generate documentation with:

```bash
poetry run sphinx-apidoc -o ./docs/ ./src/
poetry run sphinx-build -b html ./docs/ ./docs/_build/
```
Include:
Installation Guide: Steps to set up the project locally.
Usage Instructions: How to run the pipeline, configure schedules, and troubleshoot.
API Reference: Auto-generated from docstrings.
Architecture Overview: Diagrams or descriptions of the data flow and components.
Marking Criteria

The marking criteria for this coursework is as follows:


Clear definition and structuring of data pipelines (20% of Total) with clear documentation of data as per specifications.
The submitted Code respects best practices, is well organised, reproducible and flexible to cater for different usages (35% of total).
The data extraction model implementation is independent and innovative (35% of total);
Write final report based on the given instructions and structure (10% of Total).


