import os
import time
import datetime
import urllib.parse
import threading
from concurrent.futures import ThreadPoolExecutor, wait, as_completed
import csv
import pandas as pd

import requests
import urllib3
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# Disable warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ğŸš€ å–å¾—ç•¶å‰è…³æœ¬æ‰€åœ¨çš„è³‡æ–™å¤¾
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# âœ… è‡ªå‹•å°‹æ‰¾ CSV æ–‡ä»¶
# æ‰“å°å½“å‰ç›®å½•å†…å®¹ï¼Œç”¨äºè°ƒè¯•
print(f"Current directory contents: {os.listdir(BASE_DIR)}")

CSV_PATH = os.getenv("CSV_PATH", os.path.join(BASE_DIR, "equitystatic.csv"))
CSV_PATH = os.path.normpath(CSV_PATH)  # é©é… Windows & macOS/Linux
print(f"Looking for CSV at: {CSV_PATH}")

# Initialize global variables
LOG_FILENAME = f'crawler_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'

# Helper Function: Write log
def write_log(message):
    """Write log with timestamp"""
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open(LOG_FILENAME, 'a', encoding='utf-8') as f:
        f.write(f"[{timestamp}] {message}\n")

# Helper Function: Initialize Selenium WebDriver
def init_driver():
    """Initialize Selenium WebDriver"""
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
        return webdriver.Chrome(options=options)
    except Exception as e:
        write_log(f"Failed to initialize Chrome driver: {str(e)}")
        return None

# Helper Function: Get search results using selenium
def get_search_results(driver, company_name, search_url, search_query, max_trials=2):
    for trial in range(max_trials):
        try:
            driver.get(search_url)
            wait = WebDriverWait(driver, 15)
            
            search_results = wait.until(
                EC.presence_of_all_elements_located(search_query)
            )
            
            if search_results:
                return search_results
            time.sleep(1)

        except Exception as e:
            if trial < max_trials - 1:
                time.sleep(1)
                continue
            write_log(f"{company_name}: Failed to get search results: {str(e)}")
            return None
    
    return None

# Helper Function: Check PDF URL
def check_pdf_url(url):
    """æ£€æŸ¥ PDF URL æ˜¯å¦æœ‰æ•ˆ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/pdf'
        }
        response = requests.head(url, headers=headers, allow_redirects=True, timeout=5)  # å‡å°‘è¶…æ—¶æ—¶é—´
        
        if response.status_code == 200:
            final_url = response.url.lower()
            if '.pdf' in final_url:
                return True
        return False
        
    except Exception as e:
        write_log(f"Error checking URL {url}: {str(e)}")
        return False

# Helper Function: Check company name in URL
def check_company_name_in_url(url, company_name):
    """æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«å…¬å¸åç§°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰"""
    url_lower = url.lower()
    company_lower = company_name.lower()
    
    # å°†å…¬å¸åç§°æ‹†åˆ†æˆå…³é”®è¯
    company_keywords = company_lower.split()
    if not company_keywords:
        return False
        
    # ç¬¬ä¸€ä¸ªå•è¯å¿…é¡»å­˜åœ¨
    first_word = company_keywords[0]
    if first_word not in url_lower:
        return False
    
    # è®¡ç®—åŒ¹é…åº¦åˆ†æ•°
    matched_words = sum(1 for word in company_keywords if word in url_lower)
    match_score = matched_words / len(company_keywords)
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªå•è¯ï¼Œè¦æ±‚å®Œå…¨åŒ¹é…
    if len(company_keywords) == 1:
        return match_score == 1
    
    # å¦‚æœæœ‰å¤šä¸ªå•è¯ï¼Œè¦æ±‚è‡³å°‘50%çš„å•è¯åŒ¹é…
    return match_score >= 0.5

# Step 1: Try to search PDF directly in Bing
def search_pdf_in_bing(driver, company_name, year):
    # å®šä¹‰å¯èƒ½çš„æŠ¥å‘Šç±»å‹
    report_types = [
        "sustainability report",
        "CSR report",
        "ESG report",
        "ESG disclosure",
        "ESG disclosure report",
        "global impact report",
        "ESG action report",
        "corporate responsibility report",
        "environmental report",
        "sustainable development report",
        "corporate citizenship report",
        "building america report"
    ]
    
    # å°è¯•ä¸åŒçš„æŠ¥å‘Šç±»å‹æœç´¢
    for report_type in report_types:
        search_query = f"{company_name} {report_type} {year} pdf -responsibilityreports -\"annual report\""
        search_url = f"https://www.bing.com/search?q={urllib.parse.quote(search_query)}"
        write_log(f"{company_name}: Searching {report_type} for {year}")

        search_query = (By.CSS_SELECTOR, '.b_algo h2 a')
        search_results = get_search_results(driver, company_name, search_url, search_query)
        
        if not search_results:
            continue

        # æ£€æŸ¥ç»“æœ
        for result in search_results[:3]:
            url = result.get_attribute('href')
            if not url:
                continue
                
            url_lower = url.lower()
            if '.pdf' in url_lower:
                # æ’é™¤å¹´åº¦æŠ¥å‘Šç›¸å…³çš„URLï¼Œä¸åŒºåˆ†å¤§å°å†™
                excluded_terms = ['annual', 'financial', 'proxy']
                if (not any(term in url_lower for term in excluded_terms) and 
                    check_url_year(url, year) and
                    check_company_name_in_url(url, company_name) and
                    check_pdf_url(url)):
                    return url
                
    return None

# Step 2: If PDF not found directly in Bing, search company's sustainability website
def search_webpage_in_bing(driver, company_name, year):
    # å®šä¹‰å¯èƒ½çš„æŠ¥å‘Šç±»å‹
    report_types = [
        "sustainability report",
        "CSR report",
        "ESG report",
        "ESG disclosure",
        "ESG disclosure report",
        "global impact report",
        "ESG action report",
        "corporate responsibility report",
        "environmental report",
        "sustainable development report"
    ]
    
    for report_type in report_types:
        search_query = f"{company_name} {report_type} {year} -responsibilityreports"
        search_url = f"https://www.bing.com/search?q={urllib.parse.quote(search_query)}"
        write_log(f"{company_name}: Searching webpage for {report_type} {year}")
            
        search_query = (By.CSS_SELECTOR, '.b_algo h2 a')
        search_results = get_search_results(driver, company_name, search_url, search_query)

        if not search_results:
            continue
        
        url_list = []
        count = 0
        for result in search_results:
            if count >= 3:
                break
            try:
                url = result.get_attribute('href')
                if url and '.pdf' not in url.lower():
                    url_list.append(url)
                    count += 1
            except Exception as e:
                write_log(f"{company_name}: Error getting URL: {str(e)}")
                continue

        if url_list:
            return url_list

    return None

# Step 3: Find PDF links in company's sustainability website
def find_pdf_in_webpage(driver, company_name, url, year):
    write_log(f"{company_name}: Searching PDF in webpage for {year}")
    search_query = (By.TAG_NAME, "a")
    search_results = get_search_results(driver, company_name, url, search_query)

    if not search_results:
        write_log(f"{company_name}: No Search Results Found | URL: {url}")
        return None
    
    # å®šä¹‰å¯èƒ½çš„å…³é”®è¯
    keywords = [
        'sustainability',
        'sustainable',
        'csr',
        'esg',
        'esg disclosure',
        'global impact',
        'environmental',
        'responsibility',
        'responsible',
        'climate',
        'disclosure',
        'corporate citizenship',
        'citizenship',
        'building america',
        'building'
    ]
    
    # å®šä¹‰æ’é™¤è¯
    excluded_terms = ['annual', 'financial', 'proxy']
    
    for result in search_results:
        try:
            href = result.get_attribute('href')
            if not href:
                continue

            href_lower = href.lower()
            if '.pdf' in href_lower:
                # æ’é™¤ä¸ç›¸å…³çš„PDF
                if any(term in href_lower for term in excluded_terms):
                    continue
                
                # æ£€æŸ¥URLä¸­çš„å¹´ä»½å’Œå…¬å¸åç§°
                if not check_url_year(href, year):
                    continue
                    
                if not check_company_name_in_url(href, company_name):
                    continue
                    
                text = result.text.lower()
                if str(year) in text and any(keyword.lower() in text for keyword in keywords):
                    if check_pdf_url(href):
                        return href
                
        except Exception:
            continue
    
    return None

# Process single company
def process_company_year(company_name, year):
    driver = init_driver()
    if not driver:
        return None, None
    
    try:
        # Try Bing search
        pdf_url = search_pdf_in_bing(driver, company_name, year)
        if pdf_url:
            driver.quit()
            return pdf_url, 'Bing direct search'
        
        # Try Bing webpage search
        webpage_urls = search_webpage_in_bing(driver, company_name, year)
        if webpage_urls:
            for url in webpage_urls:
                pdf_url = find_pdf_in_webpage(driver, company_name, url, year)
                if pdf_url:
                    driver.quit()
                    return pdf_url, 'Bing webpage search'
                    
    except Exception as e:
        write_log(f"Error processing {company_name} for {year}: {str(e)}")
    finally:
        driver.quit()
    
    return None, 'Not found'

def process_company(company_name):
    print(f"\nProcessing company: {company_name}")
    results = []
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¹´ä»½
    with ThreadPoolExecutor(max_workers=3) as executor:  # åŒæ—¶å¤„ç†3ä¸ªå¹´ä»½
        futures = {
            executor.submit(process_company_year, company_name, year): year 
            for year in range(2019, 2025)
        }
        
        for future in as_completed(futures):
            year = futures[future]
            try:
                url, source = future.result()
                results.append({
                    'company': company_name,
                    'year': year,
                    'url': url if url else 'Not found',
                    'source': source if source else 'Not found'
                })
                print(f"  {year}: {'Found' if url else 'Not found'}")
            except Exception as e:
                write_log(f"Error processing {company_name} for {year}: {str(e)}")
                results.append({
                    'company': company_name,
                    'year': year,
                    'url': 'Not found',
                    'source': 'Not found'
                })
    
    return sorted(results, key=lambda x: x['year'])  # æŒ‰å¹´ä»½æ’åºè¿”å›ç»“æœ

def process_companies(companies, results_file):
    # Create results CSV with headers
    with open(results_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['company', 'year', 'url', 'source'])
        writer.writeheader()

    # Process each company
    for company in companies:
        company_results = process_company(company)
        
        # Save results for this company
        with open(results_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['company', 'year', 'url', 'source'])
            for result in company_results:
                writer.writerow(result)
        
        print(f"Completed processing {company}")

    print(f"\nProcessing completed. Results saved to {results_file}")

def check_url_year(url, target_year):
    """æ£€æŸ¥URLä¸­çš„å¹´ä»½æ˜¯å¦ä¸ç›®æ ‡å¹´ä»½åŒ¹é…"""
    url_lower = url.lower()
    # æå–URLä¸­çš„å¹´ä»½ï¼ˆ4ä½æ•°å­—ï¼‰
    import re
    years_in_url = re.findall(r'20[12]\d', url_lower)  # åŒ¹é…2010-2029å¹´
    
    if years_in_url:
        # å¦‚æœURLä¸­åŒ…å«å¹´ä»½ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ç›®æ ‡å¹´ä»½åŒ¹é…
        return str(target_year) in years_in_url
    return True  # å¦‚æœURLä¸­æ²¡æœ‰å¹´ä»½ï¼Œè¿”å›True

if __name__ == "__main__":
    try:
        # Read companies from CSV
        df = pd.read_csv(CSV_PATH)
        if 'security' not in df.columns:
            raise ValueError("CSV file must contain a 'security' column")
        
        # éšæœºæŠ½å–5å®¶å…¬å¸
        sample_companies = df['security'].sample(n=5).tolist()
        print(f"Randomly selected companies: {sample_companies}")
        
        # åˆ›å»ºç»“æœæ–‡ä»¶å
        results_file = os.path.join(BASE_DIR, f'sample_results_{datetime.datetime.now().strftime("%Y%m%d")}.csv')
        
        # Process companies
        process_companies(sample_companies, results_file)
        
        # æ£€æŸ¥æ¯ä¸ªå…¬å¸çš„æœ‰æ•ˆURLæ•°é‡
        df_results = pd.read_csv(results_file)
        for company in sample_companies:
            valid_urls = df_results[
                (df_results['company'] == company) & 
                (df_results['url'] != 'Not found')
            ].shape[0]
            
            status = 'SUCCESS' if valid_urls >= 2 else 'FAILED'
            print(f"{company}: {valid_urls} valid URLs - {status}")
        
    except FileNotFoundError:
        print("Error: equitystatic.csv not found")
    except Exception as e:
        print(f"Error: {str(e)}")
