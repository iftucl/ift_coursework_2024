import os
import aiohttp
import asyncio
import pandas as pd
import random
import time
from tqdm.asyncio import tqdm
import nest_asyncio  # å…¼å®¹ Jupyter Notebook

nest_asyncio.apply()  # é€‚ç”¨äº Jupyter Notebook

BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # å–å¾—ç•¶å‰è…³æœ¬æ‰€åœ¨ç›®éŒ„
# è®¾ç½®ä¸‹è½½ç›®å½•ä¸º b_pipeline/test/downloaded_pdfs
DOWNLOAD_ROOT = os.getenv("DOWNLOAD_PATH", os.path.join(BASE_DIR, "downloaded_pdfs"))
os.makedirs(DOWNLOAD_ROOT, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨

# æ‰“å°è·¯å¾„ç”¨äºè°ƒè¯•
print(f"Files will be downloaded to: {DOWNLOAD_ROOT}")

# è¯»å– CSV æ–‡ä»¶
CSV_PATH = os.getenv("CSV_PATH", os.path.join(BASE_DIR, "cleaned_url.csv"))
print(f"Looking for CSV at: {CSV_PATH}")

if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° `{CSV_PATH}`ï¼Œè«‹ç¢ºèªæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼")

df = pd.read_csv(CSV_PATH)

# ç¡®ä¿éœ€è¦çš„åˆ—å­˜åœ¨
required_columns = {"company", "year", "url"}
if not required_columns.issubset(df.columns):
    raise ValueError(f"CSV æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns - set(df.columns)}")

# éšæœºæŠ½å– 30 ä¸ª URL
df_sample = df.sample(n=30, random_state=42)

# é™åˆ¶å¹¶å‘æ•°ï¼ˆå¯è°ƒæ•´ï¼‰
MAX_CONCURRENT_DOWNLOADS = 10 # ä¸€æ¬¡ä¸‹è½½ä¸€ä¸ª
semaphore = asyncio.Semaphore(MAX_CONCURRENT_DOWNLOADS)

# ç»Ÿè®¡ä¸‹è½½æ—¶é—´
download_times = []

# å¤±è´¥æ—¥å¿—æ–‡ä»¶ (TXT æ ¼å¼)
LOG_FILE = "download_failed.txt"

# æ·»åŠ  User-Agent & ä¼ªè£… Referer
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",  # Edgeæµè§ˆå™¨çš„UA
    "Referer": "https://www.bing.com/search",
    "Accept": "application/pdf,application/octet-stream",
    "Accept-Language": "en-US,en;q=0.9",
    "Connection": "keep-alive"
}

def log_failed_download(url, reason):
    """è®°å½•ä¸‹è½½å¤±è´¥çš„ URL å’Œé”™è¯¯åŸå› åˆ° TXT æ–‡ä»¶"""
    with open(LOG_FILE, "a", encoding="utf-8") as log_file:
        log_file.write(f"Failed: {url} | Reason: {reason}\n")

def log_statistics(stats_info):
    """å°†ç»Ÿè®¡ä¿¡æ¯å†™å…¥æ—¥å¿—æ–‡ä»¶"""
    with open(LOG_FILE, "a", encoding="utf-8") as log_file:
        log_file.write("\n" + "="*50 + "\n")
        log_file.write("ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯\n")
        log_file.write("="*50 + "\n")
        log_file.write(stats_info)
        log_file.write("\n" + "="*50 + "\n")

async def download_pdf(session, row, overall_progress):
    """ä¸‹è½½ PDF å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ï¼Œå¸¦è¿›åº¦æ¡ï¼Œå¹¶è®°å½•å¤±è´¥æ—¥å¿—"""
    company = row['company']
    year = str(row['year'])
    url = row['url']
    
    save_dir = os.path.join(DOWNLOAD_ROOT, company, year)
    save_path = os.path.join(save_dir, f"{company}_{year}.pdf")

    print(f"\nå°è¯•ä¸‹è½½: {company} {year}")
    print(f"URL: {url}")

    async with semaphore:
        start_time = time.time()

        try:
            async with session.get(url, headers=HEADERS) as response:
                if response.status == 200:
                    total_size = int(response.headers.get("content-length", 0))
                    chunk_size = 1024
                    downloaded = 0

                    # åˆ›å»ºå•ä¸ªæ–‡ä»¶çš„è¿›åº¦æ¡
                    with tqdm(total=total_size, unit='B', unit_scale=True, 
                            desc=f"ä¸‹è½½ {company}_{year}", leave=True) as file_progress:
                        
                        # åªæœ‰åœ¨ HTTP 200 æ—¶æ‰åˆ›å»ºç›®å½•
                        os.makedirs(save_dir, exist_ok=True)

                        with open(save_path, "wb") as f:
                            async for chunk in response.content.iter_chunked(chunk_size):
                                f.write(chunk)
                                downloaded += len(chunk)
                                file_progress.update(len(chunk))

                    download_time = time.time() - start_time
                    download_times.append({
                        'company': company,
                        'year': year,
                        'time': download_time,
                        'size': total_size
                    })
                    
                    speed = total_size / download_time / 1024 / 1024  # MB/s
                    print(f"âœ… æˆåŠŸä¸‹è½½: {save_path}")
                    print(f"   è€—æ—¶: {download_time:.1f}ç§’, é€Ÿåº¦: {speed:.2f}MB/s")
                    
                    overall_progress.update(1)
                    return True
                else:
                    error_msg = f"HTTP {response.status}"
                    print(f"âŒ ä¸‹è½½å¤±è´¥: {error_msg}")
                    log_failed_download(url, error_msg)
                    return False

        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½é”™è¯¯: {str(e)}")
            log_failed_download(url, str(e))
            return False

async def main():
    start_time = time.time()
    success_count = 0
    failed_count = 0
    
    async with aiohttp.ClientSession() as session:
        total_files = len(df_sample)
        
        # åˆ›å»ºæ€»è¿›åº¦æ¡
        with tqdm(total=total_files, desc="æ€»è¿›åº¦", unit="file") as overall_progress:
            for _, row in df_sample.iterrows():
                try:
                    result = await download_pdf(session, row, overall_progress)
                    if result:
                        success_count += 1
                    else:
                        failed_count += 1
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    await asyncio.sleep(1)
                except Exception as e:
                    print(f"å¤„ç†é”™è¯¯: {str(e)}")
                    failed_count += 1
                    continue
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        print("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"æˆåŠŸä¸‹è½½: {success_count}")
        print(f"ä¸‹è½½å¤±è´¥: {failed_count}")
        print(f"æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        if download_times:
            # è®¡ç®—æ¯ä¸ªå¹´ä»½çš„å¹³å‡ä¸‹è½½æ—¶é—´
            year_stats = {}
            for item in download_times:
                year = item['year']
                if year not in year_stats:
                    year_stats[year] = {'times': [], 'sizes': []}
                year_stats[year]['times'].append(item['time'])
                year_stats[year]['sizes'].append(item['size'])
            
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            stats = []
            stats.append("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
            stats.append(f"æ€»æ–‡ä»¶æ•°: {total_files}")
            stats.append(f"æˆåŠŸä¸‹è½½: {success_count}")
            stats.append(f"ä¸‹è½½å¤±è´¥: {failed_count}")
            stats.append(f"æ€»è€—æ—¶: {total_time:.1f}ç§’")
            
            if download_times:
                # å„å¹´ä»½ç»Ÿè®¡
                stats.append("\nğŸ“ˆ å„å¹´ä»½ä¸‹è½½ç»Ÿè®¡:")
                for year in sorted(year_stats.keys()):
                    times = year_stats[year]['times']
                    sizes = year_stats[year]['sizes']
                    avg_time = sum(times) / len(times)
                    total_size = sum(sizes) / (1024 * 1024)
                    stats.append(f"\n{year}å¹´:")
                    stats.append(f"  - æˆåŠŸä¸‹è½½æ•°é‡: {len(times)}ä»½")
                    stats.append(f"  - å¹³å‡ä¸‹è½½æ—¶é—´: {avg_time:.1f}ç§’")
                    stats.append(f"  - æ€»ä¸‹è½½å¤§å°: {total_size:.1f}MB")
                    stats.append(f"  - å¹³å‡ä¸‹è½½é€Ÿåº¦: {total_size/sum(times):.2f}MB/s")
                
                # æ€»ä½“ç»Ÿè®¡
                stats.append("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
                avg_time = sum(item['time'] for item in download_times) / len(download_times)
                total_size = sum(item['size'] for item in download_times) / (1024 * 1024)
                stats.append(f"å¹³å‡ä¸‹è½½æ—¶é—´: {avg_time:.1f}ç§’")
                stats.append(f"æ€»ä¸‹è½½å¤§å°: {total_size:.1f}MB")
                stats.append(f"å¹³å‡ä¸‹è½½é€Ÿåº¦: {total_size/total_time:.2f}MB/s")
                
                # æœ€å¿«æœ€æ…¢è®°å½•
                fastest = min(download_times, key=lambda x: x['time'])
                slowest = max(download_times, key=lambda x: x['time'])
                stats.append(f"\nâš¡ï¸ æœ€å¿«ä¸‹è½½: {fastest['company']} ({fastest['year']}) - {fastest['time']:.1f}ç§’")
                stats.append(f"ğŸŒ æœ€æ…¢ä¸‹è½½: {slowest['company']} ({slowest['year']}) - {slowest['time']:.1f}ç§’")

            # å°†ç»Ÿè®¡ä¿¡æ¯å†™å…¥æ—¥å¿—æ–‡ä»¶
            stats_text = "\n".join(stats)
            print(stats_text)  # æ‰“å°åˆ°æ§åˆ¶å°
            log_statistics(stats_text)  # å†™å…¥æ—¥å¿—æ–‡ä»¶
        
        # æ£€æŸ¥ä¸‹è½½å¤±è´¥æ—¥å¿—
        if os.path.exists(LOG_FILE) and os.path.getsize(LOG_FILE) > 0:
            print("\nâš ï¸ ä¸‹è½½å¤±è´¥çš„è®°å½•:")
            with open(LOG_FILE, 'r') as f:
                print(f.read())

if __name__ == "__main__":
    asyncio.run(main())
